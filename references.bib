@book{alfred2007compilers,
  title = {Compilers: {{Principles}}, {{Techniques}} \& {{Tools}}},
  author = {Alfred, V Aho and Monica, S Lam and Jeffrey, D Ullman},
  date = {2007},
  edition = {2},
  publisher = {pearson Education}
}

@inproceedings{amdahlValiditySingleProcessor1967,
  title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
  booktitle = {Proceedings of the {{April}} 18-20, 1967, Spring Joint Computer Conference},
  author = {Amdahl, Gene M.},
  date = {1967-04-18},
  series = {{{AFIPS}} '67 ({{Spring}})},
  pages = {483--485},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1465482.1465560},
  url = {https://dl.acm.org/doi/10.1145/1465482.1465560},
  urldate = {2025-01-13},
  abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.},
  isbn = {978-1-4503-7895-6},
  file = {/Users/edjg/Zotero/storage/VRNQ8ZCL/Amdahl - 1967 - Validity of the single processor approach to achieving large scale computing capabilities.pdf}
}

@unpublished{aminiHowSlowMLIR2024,
  title = {How {{Slow}} Is {{MLIR}}?},
  author = {Amini, Mehdi and Nui, Jeff},
  date = {2024-04},
  url = {https://www.youtube.com/watch?v=7qvVMUSxqz4},
  eventtitle = {2024 {{European LLVM Developers}}' {{Meeting}}},
  venue = {Vienna, Austria}
}

@article{aycockBriefHistoryJustintime2003,
  title = {A Brief History of Just-in-Time},
  author = {Aycock, John},
  date = {2003-06-01},
  journaltitle = {ACM Comput. Surv.},
  volume = {35},
  number = {2},
  pages = {97--113},
  issn = {0360-0300},
  doi = {10.1145/857076.857077},
  url = {https://dl.acm.org/doi/10.1145/857076.857077},
  urldate = {2025-05-11},
  abstract = {Software systems have been using "just-in-time" compilation (JIT) techniques since the 1960s. Broadly, JIT compilation includes any translation performed dynamically, after a program has started execution. We examine the motivation behind JIT compilation and constraints imposed on JIT compilation systems, and present a classification scheme for such systems. This classification emerges as we survey forty years of JIT work, from 1960--2000.},
  file = {/Users/edjg/Zotero/storage/KFW9CF8P/Aycock - 2003 - A brief history of just-in-time.pdf}
}

@inproceedings{baconFastStaticAnalysis1996,
  title = {Fast Static Analysis of {{C}}++ Virtual Function Calls},
  booktitle = {Proceedings of the 11th {{ACM SIGPLAN}} Conference on {{Object-oriented}} Programming, Systems, Languages, and Applications},
  author = {Bacon, David F. and Sweeney, Peter F.},
  date = {1996-10-01},
  series = {{{OOPSLA}} '96},
  pages = {324--341},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/236337.236371},
  url = {https://dl.acm.org/doi/10.1145/236337.236371},
  urldate = {2025-05-11},
  abstract = {Virtual functions make code easier for programmers to reuse but also make it harder for compilers to analyze. We investigate the ability of three static analysis algorithms to improve C++ programs by resolving virtual function calls, thereby reducing compiled code size and reducing program complexity so as to improve both human and automated program understanding and analysis. In measurements of seven programs of significant size (5000 to 20000 lines of code each) we found that on average the most precise of the three algorithms resolved 71\% of the virtual function calls and reduced compiled code size by 25\%. This algorithm is very fast: it analyzes 3300 source lines per second on an 80 MHz PowerPC 601. Because of its accuracy and speed, this algorithm is an excellent candidate for inclusion in production C++ compilers.},
  isbn = {978-0-89791-788-9},
  file = {/Users/edjg/Zotero/storage/LL8LCXSD/Bacon and Sweeney - 1996 - Fast static analysis of C++ virtual function calls.pdf}
}

@article{barrettVirtualMachineWarmup2017,
  title = {Virtual {{Machine Warmup Blows Hot}} and {{Cold}}},
  author = {Barrett, Edd and Bolz-Tereick, Carl Friedrich and Killick, Rebecca and Mount, Sarah and Tratt, Laurence},
  date = {2017-10-12},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {1},
  eprint = {1602.00602},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--27},
  issn = {2475-1421},
  doi = {10.1145/3133876},
  url = {http://arxiv.org/abs/1602.00602},
  urldate = {2025-05-14},
  abstract = {Virtual Machines (VMs) with Just-In-Time (JIT) compilers are traditionally thought to execute programs in two phases: the initial warmup phase determines which parts of a program would most benefit from dynamic compilation, before JIT compiling those parts into machine code; subsequently the program is said to be at a steady state of peak performance. Measurement methodologies almost always discard data collected during the warmup phase such that reported measurements focus entirely on peak performance. We introduce a fully automated statistical approach, based on changepoint analysis, which allows us to determine if a program has reached a steady state and, if so, whether that represents peak performance or not. Using this, we show that even when run in the most controlled of circumstances, small, deterministic, widely studied microbenchmarks often fail to reach a steady state of peak performance on a variety of common VMs. Repeating our experiment on 3 different machines, we found that at most 43.5\% of {$<$}VM, benchmark{$>$} pairs consistently reach a steady state of peak performance.},
  issue = {OOPSLA},
  keywords = {Computer Science - Programming Languages},
  file = {/Users/edjg/Zotero/storage/MR6M8M8V/Barrett et al. - 2017 - Virtual Machine Warmup Blows Hot and Cold.pdf;/Users/edjg/Zotero/storage/U9VBGS8S/1602.html}
}

@online{bastienabadieEngineeringCodeQuality,
  title = {Engineering Code Quality in the {{Firefox}} Browser: {{A}} Look at Our Tools and Challenges},
  shorttitle = {Engineering Code Quality in the {{Firefox}} Browser},
  author = {{Bastien Abadie} and {Sylvestre Ledru}},
  url = {https://hacks.mozilla.org/2020/04/code-quality-tools-at-mozilla},
  urldate = {2025-05-13},
  abstract = {Here's an insider's look at Firefox's code quality toolchain that's been designed to manage the ongoing development and monthly releases of our desktop browser. This post explores the architecture, challenges, ...},
  langid = {american},
  organization = {Mozilla Hacks – the Web developer blog},
  file = {/Users/edjg/Zotero/storage/PS3AQWI2/code-quality-tools-at-mozilla.html}
}

@inproceedings{bergerTriangulatingPythonPerformance2023a,
  title = {Triangulating {{Python Performance Issues}} with {{SCALENE}}},
  author = {Berger, Emery D. and Stern, Sam and Pizzorno, Juan Altmayer},
  date = {2023},
  pages = {51--64},
  url = {https://www.usenix.org/conference/osdi23/presentation/berger},
  urldate = {2025-05-14},
  eventtitle = {17th {{USENIX Symposium}} on {{Operating Systems Design}} and {{Implementation}} ({{OSDI}} 23)},
  isbn = {978-1-939133-34-2},
  langid = {english},
  file = {/Users/edjg/Zotero/storage/2PSMWKHJ/Berger et al. - 2023 - Triangulating Python Performance Issues with SCALENE .pdf}
}

@inproceedings{callahanInterproceduralConstantPropagation1986,
  title = {Interprocedural Constant Propagation},
  booktitle = {Proceedings of the 1986 {{SIGPLAN}} Symposium on {{Compiler}} Construction},
  author = {Callahan, David and Cooper, Keith D. and Kennedy, Ken and Torczon, Linda},
  date = {1986-07-01},
  series = {{{SIGPLAN}} '86},
  pages = {152--161},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/12276.13327},
  url = {https://dl.acm.org/doi/10.1145/12276.13327},
  urldate = {2025-05-16},
  abstract = {In a compiling system that attempts to improve code for a whole program by optimizing across procedures, the compiler can generate better code for a specific procedure if it knows which variables will have constant values, and what those values will be, when the procedure is invoked. This paper presents a general algorithm for determining for each procedure in a given program the set of inputs that will have known constant values at run time. The precision of the answers provided by this method are dependent on the precision of the local analysis of individual procedures in the program. Since the algorithm is intended for use in a sophisticated software development environment in which local analysis would be provided by the source editor, the quality of the answers will depend on the amount of work the editor performs. Several reasonable strategies for local analysis with different levels of complexity and precision are suggested and the results of a prototype implementation in a vectorizing Fortran compiler are presented.},
  isbn = {978-0-89791-197-9},
  file = {/Users/edjg/Zotero/storage/N39QA3AE/Callahan et al. - 1986 - Interprocedural constant propagation.pdf}
}

@incollection{chatleyNext7000Programming2019,
  title = {The {{Next}} 7000 {{Programming Languages}}},
  booktitle = {Computing and {{Software Science}}: {{State}} of the {{Art}} and {{Perspectives}}},
  author = {Chatley, Robert and Donaldson, Alastair and Mycroft, Alan},
  editor = {Steffen, Bernhard and Woeginger, Gerhard},
  date = {2019},
  pages = {250--282},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-91908-9_15},
  url = {https://doi.org/10.1007/978-3-319-91908-9_15},
  urldate = {2025-05-10},
  abstract = {Landin’s seminal paper “The next 700 programming languages” considered programming languages prior to 1966 and speculated on the next 700. Half-a-century on, we cast programming languages in a Darwinian ‘tree of life’ and explore languages, their features (genes) and language evolution from the viewpoint of ‘survival of the fittest’.},
  isbn = {978-3-319-91908-9},
  langid = {english},
  file = {/Users/edjg/Zotero/storage/8ZWWCKTL/Chatley et al. - 2019 - The Next 7000 Programming Languages.pdf}
}

@software{collinwinterPythonPyperformance2025,
  title = {Python/Pyperformance},
  author = {{Collin Winter} and {Jeffrey Yasskin}},
  date = {2025-05-06T16:31:43Z},
  origdate = {2016-08-17T23:43:22Z},
  url = {https://github.com/python/pyperformance},
  urldate = {2025-05-15},
  abstract = {Python Performance Benchmark Suite},
  organization = {Python},
  keywords = {benchmark,performance,python}
}

@article{cooperAdaptiveOptimizingCompilers2002,
  title = {Adaptive {{Optimizing Compilers}} for the 21st {{Century}}},
  author = {Cooper, Keith D. and Subramanian, Devika and Torczon, Linda},
  date = {2002-08-01},
  journaltitle = {The Journal of Supercomputing},
  shortjournal = {The Journal of Supercomputing},
  volume = {23},
  number = {1},
  pages = {7--22},
  issn = {1573-0484},
  doi = {10.1023/A:1015729001611},
  url = {https://doi.org/10.1023/A:1015729001611},
  urldate = {2025-05-12},
  abstract = {Historically, compilers have operated by applying a fixed set of optimizations in a predetermined order. We call such an ordered list of optimizations a compilation sequence. This paper describes a prototype system that uses biased random search to discover a program-specific compilation sequence that minimizes an explicit, external objective function. The result is a compiler framework that adapts its behavior to the application being compiled, to the pool of available transformations, to the objective function, and to the target machine.},
  langid = {english},
  keywords = {biased random search,configurable compilers,optimizing compilers,order of optimization},
  file = {/Users/edjg/Zotero/storage/YHJIJZQ4/Cooper et al. - 2002 - Adaptive Optimizing Compilers for the 21st Century.pdf}
}

@thesis{crapeperformance,
  type = {Master's dissertation},
  title = {Performance Analysis and Benchmarking of {{Python}}},
  author = {Crapé, Arthur},
  date = {2020},
  institution = {Ghent University},
  location = {Ghent, Belgium},
  pagetotal = {105},
  file = {/Users/edjg/Zotero/storage/W2EIY5QW/Crapé - Performance analysis and benchmarking of python.pdf}
}

@article{curtsingerSTABILIZERStatisticallySound2013,
  title = {{{STABILIZER}}: Statistically Sound Performance Evaluation},
  shorttitle = {{{STABILIZER}}},
  author = {Curtsinger, Charlie and Berger, Emery D.},
  date = {2013-03-16},
  journaltitle = {SIGARCH Comput. Archit. News},
  volume = {41},
  number = {1},
  pages = {219--228},
  issn = {0163-5964},
  doi = {10.1145/2490301.2451141},
  url = {https://dl.acm.org/doi/10.1145/2490301.2451141},
  urldate = {2025-01-29},
  abstract = {Researchers and software developers require effective performance evaluation. Researchers must evaluate optimizations or measure overhead. Software developers use automatic performance regression tests to discover when changes improve or degrade performance. The standard methodology is to compare execution times before and after applying changes.Unfortunately, modern architectural features make this approach unsound. Statistically sound evaluation requires multiple samples to test whether one can or cannot (with high confidence) reject the null hypothesis that results are the same before and after. However, caches and branch predictors make performance dependent on machine-specific parameters and the exact layout of code, stack frames, and heap objects. A single binary constitutes just one sample from the space of program layouts, regardless of the number of runs. Since compiler optimizations and code changes also alter layout, it is currently impossible to distinguish the impact of an optimization from that of its layout effects.This paper presents Stabilizer, a system that enables the use of the powerful statistical techniques required for sound performance evaluation on modern architectures. Stabilizer forces executions to sample the space of memory configurations by repeatedly re-randomizing layouts of code, stack, and heap objects at runtime. Stabilizer thus makes it possible to control for layout effects. Re-randomization also ensures that layout effects follow a Gaussian distribution, enabling the use of statistical tests like ANOVA. We demonstrate Stabilizer's efficiency (\&lt;7\% median overhead) and its effectiveness by evaluating the impact of LLVM's optimizations on the SPEC CPU2006 benchmark suite. We find that, while -O2 has a significant impact relative to -O1, the performance impact of -O3 over -O2 optimizations is indistinguishable from random noise.},
  file = {/Users/edjg/Zotero/storage/P6T7ZTT6/Curtsinger and Berger - 2013 - STABILIZER statistically sound performance evaluation.pdf}
}

@unpublished{emerybergerPythonPerformanceMatters2022,
  title = {"{{Python Performance Matters}}" by {{Emery Berger}} ({{Strange Loop}} 2022)},
  author = {{Emery Berger}},
  date = {2022-10-06},
  url = {https://www.youtube.com/watch?v=vVUnCXKuNOg},
  urldate = {2024-10-21},
  abstract = {It's 2022. Moore's Law and Dennard scaling have run out of steam, making it harder than ever to achieve high performance - especially in Python. This talk first explains in detail the unique challenges that Python poses to programmers. It then presents Scalene, a novel high-performance CPU, GPU and memory profiler for Python that does many things that past Python profilers do not and cannot do. Scalene both runs orders of magnitude faster than other profilers while delivering more accurate and more actionable information that's especially valuable to Python programmers. Emery Berger Professor, University of Massachusetts Amherst @emeryberger Emery Berger is a Professor of Computer Sciences at the University of Massachusetts Amherst, the flagship campus of the UMass system. Professor Berger and his collaborators have built numerous widely adopted software systems including Hoard, a fast and scalable memory manager that accelerates multithreaded applications (on which the Mac OS X memory manager is based); DieHard/DieHarder, error-avoiding and secure memory managers that influenced Windows, and Coz, a "causal profiler" that ships with modern Linux distros. He is also the developer and maintainer of CSrankings.org. His honors include an NSF CAREER Award, Most Influential Paper Awards at OOPSLA, at PLDI, and ASPLOS; five CACM Research Highlights, and Best Paper Awards at FAST, OOPSLA, and SOSP; he is an ACM Fellow. Professor Berger served six years as an elected member of the SIGPLAN Executive Committee; a decade as Associate Editor of TOPLAS; he was Program Chair for PLDI 2016 and co-Program Chair of ASPLOS 2021. ------- Sponsored by: ------- Stream is the \# 1 Chat API for custom messaging apps. Activate your free 30-day trial to explore Stream Chat. https://gstrm.io/tsl},
  venue = {Strange Loop Conference}
}

@inproceedings{fehrIRDLIRDefinition2022a,
  title = {{{IRDL}}: An {{IR}} Definition Language for {{SSA}} Compilers},
  shorttitle = {{{IRDL}}},
  booktitle = {Proceedings of the 43rd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Fehr, Mathieu and Niu, Jeff and Riddle, River and Amini, Mehdi and Su, Zhendong and Grosser, Tobias},
  date = {2022-06-09},
  series = {{{PLDI}} 2022},
  pages = {199--212},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3519939.3523700},
  url = {https://dl.acm.org/doi/10.1145/3519939.3523700},
  urldate = {2025-04-11},
  abstract = {Designing compiler intermediate representations (IRs) is often a manual process that makes exploration and innovation in this space costly. Developers typically use general-purpose programming languages to design IRs. As a result, IR implementations are verbose, manual modifications are expensive, and designing tooling for the inspection or generation of IRs is impractical. While compilers relied historically on a few slowly evolving IRs, domain-specific optimizations and specialized hardware motivate compilers to use and evolve many IRs. We facilitate the implementation of SSA-based IRs by introducing IRDL, a domain-specific language to define IRs. We analyze all 28 domain-specific IRs developed as part of LLVM's MLIR project over the last two years and demonstrate how to express these IRs exclusively in IRDL while only rarely falling back to IRDL's support for generic C++ extensions. By enabling the concise and explicit specification of IRs, we provide foundations for developing effective tooling to automate the compiler construction process.},
  isbn = {978-1-4503-9265-5},
  file = {/Users/edjg/Zotero/storage/VB4Z3UNB/Fehr et al. - 2022 - IRDL an IR definition language for SSA compilers.pdf}
}

@software{fehrXdslprojectXdsl2024,
  title = {Xdslproject/Xdsl},
  author = {Fehr, Mathieu and Lopoukhine, Alexandre},
  date = {2024-10-21T13:01:34Z},
  origdate = {2021-09-12T14:02:30Z},
  url = {https://github.com/xdslproject/xdsl},
  urldate = {2024-10-21},
  abstract = {A Python Compiler Design Toolkit},
  organization = {xdslproject}
}

@inproceedings{fehrXDSLSidekickCompilation2025,
  title = {{{xDSL}}: {{Sidekick Compilation}} for {{SSA-Based Compilers}}},
  shorttitle = {{{xDSL}}},
  booktitle = {Proceedings of the 23rd {{ACM}}/{{IEEE International Symposium}} on {{Code Generation}} and {{Optimization}}},
  author = {Fehr, Mathieu and Weber, Michel and Ulmann, Christian and Lopoukhine, Alexandre and Lücke, Martin Paul and Degioanni, Théo and Vasiladiotis, Christos and Steuwer, Michel and Grosser, Tobias},
  date = {2025-03-01},
  series = {{{CGO}} '25},
  pages = {179--192},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3696443.3708945},
  url = {https://dl.acm.org/doi/10.1145/3696443.3708945},
  urldate = {2025-04-11},
  abstract = {Traditionally, compiler researchers either conduct experiments within an existing production compiler or develop their own prototype compiler; both options come with trade-offs.                                On one hand, prototyping in a production compiler can be cumbersome, as they are often optimized for program compilation speed at the expense of software simplicity and development speed.                                On the other hand, the transition from a prototype compiler to production requires significant engineering work.                                To bridge this gap, we introduce the concept of sidekick compiler frameworks, an approach that uses multiple frameworks that interoperate with each other by leveraging textual interchange formats and declarative descriptions of abstractions.                                Each such compiler framework is specialized for specific use cases, such as performance or prototyping.                                Abstractions are by design shared across frameworks, simplifying the transition from prototyping to production.                                We demonstrate this idea with xDSL, a sidekick for MLIR focused on prototyping and teaching.                                xDSL interoperates with MLIR through a shared textual IR and the exchange of IRs through an IR Definition Language.                                The benefits of sidekick compiler frameworks are evaluated by showing on three use cases how xDSL impacts their development: teaching, DSL compilation, and rewrite system prototyping.                                We also investigate the trade-offs that xDSL offers, and demonstrate how we simplify the transition between frameworks using the IRDL dialect.                                With sidekick compilation, we envision a future in which engineers minimize the cost of development by choosing a framework built for their immediate needs, and later transitioning to production with minimal overhead.},
  isbn = {979-8-4007-1275-3},
  file = {/Users/edjg/Zotero/storage/7YEB29W8/Fehr et al. - 2025 - xDSL Sidekick Compilation for SSA-Based Compilers.pdf}
}

@software{gaoVizTracer2025,
  title = {{{VizTracer}}},
  author = {Gao, Tian},
  date = {2025-04-28T14:34:17Z},
  origdate = {2020-08-05T00:29:56Z},
  url = {https://github.com/gaogaotiantian/viztracer},
  urldate = {2025-04-28},
  abstract = {A debugging and profiling tool that can trace and visualize python code execution}
}

@software{guidovanrossumPythonCpython2025,
  title = {Python/Cpython},
  author = {{Guido van Rossum}},
  date = {2025-04-28T14:44:57Z},
  origdate = {2017-02-10T19:23:51Z},
  url = {https://github.com/python/cpython},
  urldate = {2025-04-28},
  abstract = {The Python programming language},
  organization = {Python}
}

@incollection{harris2021understanding,
  title = {Understanding Computation Time: A Critical Discussion of Time as a Computational Performance Metric},
  booktitle = {Time in Variance},
  author = {Harris-Birtill, David and Harris-Birtill, Rose},
  date = {2021},
  pages = {220--248},
  publisher = {Brill},
  file = {/Users/edjg/Zotero/storage/S7HA4J9C/Harris-Birtill and Harris-Birtill - 2021 - Understanding computation time a critical discussion of time as a computational performance metric.pdf}
}

@inproceedings{holzleOptimizingDynamicallydispatchedCalls1994,
  title = {Optimizing Dynamically-Dispatched Calls with Run-Time Type Feedback},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 1994 Conference on {{Programming}} Language Design and Implementation},
  author = {Hölzle, Urs and Ungar, David},
  date = {1994-06-01},
  series = {{{PLDI}} '94},
  pages = {326--336},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/178243.178478},
  url = {https://dl.acm.org/doi/10.1145/178243.178478},
  urldate = {2025-05-12},
  isbn = {978-0-89791-662-2},
  file = {/Users/edjg/Zotero/storage/4AMQL6SQ/Hölzle and Ungar - 1994 - Optimizing dynamically-dispatched calls with run-time type feedback.pdf}
}

@online{joshhabermanTailCallingInterpreter2025,
  title = {A {{Tail Calling Interpreter For Python}} ({{And Other Updates}})},
  author = {{Josh Haberman}},
  date = {2025-10-02},
  url = {https://blog.reverberate.org/2025/02/10/tail-call-updates.html},
  urldate = {2025-05-14},
  file = {/Users/edjg/Zotero/storage/LGPDYQWY/tail-call-updates.html}
}

@online{kempenItsNotEasy2025,
  title = {It's {{Not Easy Being Green}}: {{On}} the {{Energy Efficiency}} of {{Programming Languages}}},
  shorttitle = {It's {{Not Easy Being Green}}},
  author = {family=Kempen, given=Nicolas, prefix=van, useprefix=false and Kwon, Hyuk-Je and Nguyen, Dung Tuan and Berger, Emery D.},
  date = {2025-03-28},
  eprint = {2410.05460},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.05460},
  url = {http://arxiv.org/abs/2410.05460},
  urldate = {2025-05-11},
  abstract = {Does the choice of programming language affect energy consumption? Previous highly visible studies have established associations between certain programming languages and energy consumption. A causal misinterpretation of this work has led academics and industry leaders to use or support certain languages based on their claimed impact on energy consumption. This paper tackles this causal question directly. It first corrects and improves the measurement methodology used by prior work. It then develops a detailed causal model capturing the complex relationship between programming language choice and energy consumption. This model identifies and incorporates several critical but previously overlooked factors that affect energy usage. These factors, such as distinguishing programming languages from their implementations, the impact of the application implementations themselves, the number of active cores, and memory activity, can significantly skew energy consumption measurements if not accounted for. We show -- via empirical experiments, improved methodology, and careful examination of anomalies -- that when these factors are controlled for, notable discrepancies in prior work vanish. Our analysis suggests that the choice of programming language implementation has no significant impact on energy consumption beyond execution time.},
  pubstate = {prepublished},
  keywords = {Computer Science - Performance,Computer Science - Programming Languages},
  file = {/Users/edjg/Zotero/storage/C65BSPZP/Kempen et al. - 2025 - It's Not Easy Being Green On the Energy Efficiency of Programming Languages.pdf;/Users/edjg/Zotero/storage/WLEKD4IA/2410.html}
}

@inproceedings{laaberEvaluationOpensourceSoftware2018,
  title = {An Evaluation of Open-Source Software Microbenchmark Suites for Continuous Performance Assessment},
  booktitle = {Proceedings of the 15th {{International Conference}} on {{Mining Software Repositories}}},
  author = {Laaber, Christoph and Leitner, Philipp},
  date = {2018-05-28},
  series = {{{MSR}} '18},
  pages = {119--130},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3196398.3196407},
  url = {https://dl.acm.org/doi/10.1145/3196398.3196407},
  urldate = {2025-04-24},
  abstract = {Continuous integration (CI) emphasizes quick feedback to developers. This is at odds with current practice of performance testing, which predominantely focuses on long-running tests against entire systems in production-like environments. Alternatively, software microbenchmarking attempts to establish a performance baseline for small code fragments in short time. This paper investigates the quality of microbenchmark suites with a focus on suitability to deliver quick performance feedback and CI integration. We study ten open-source libraries written in Java and Go with benchmark suite sizes ranging from 16 to 983 tests, and runtimes between 11 minutes and 8.75 hours. We show that our study subjects include benchmarks with result variability of 50\% or higher, indicating that not all benchmarks are useful for reliable discovery of slowdowns. We further artificially inject actual slowdowns into public API methods of the study subjects and test whether test suites are able to discover them. We introduce a performance-test quality metric called the API benchmarking score (ABS). ABS represents a benchmark suite's ability to find slowdowns among a set of defined core API methods. Resulting benchmarking scores (i.e., fraction of discovered slowdowns) vary between 10\% and 100\% for the study subjects. This paper's methodology and results can be used to (1) assess the quality of existing microbenchmark suites, (2) select a set of tests to be run as part of CI, and (3) suggest or generate benchmarks for currently untested parts of an API.},
  isbn = {978-1-4503-5716-6},
  file = {/Users/edjg/Zotero/storage/8MD74V8C/Laaber and Leitner - 2018 - An evaluation of open-source software microbenchmark suites for continuous performance assessment.pdf}
}

@inproceedings{lamNumbaLLVMbasedPython2015,
  title = {Numba: A {{LLVM-based Python JIT}} Compiler},
  shorttitle = {Numba},
  booktitle = {Proceedings of the {{Second Workshop}} on the {{LLVM Compiler Infrastructure}} in {{HPC}}},
  author = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
  date = {2015-11-15},
  series = {{{LLVM}} '15},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2833157.2833162},
  url = {https://dl.acm.org/doi/10.1145/2833157.2833162},
  urldate = {2025-05-11},
  abstract = {Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is often a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addition, we share our experience in building a JIT compiler using LLVM[1].},
  isbn = {978-1-4503-4005-2},
  file = {/Users/edjg/Zotero/storage/YIM4DLPJ/Lam et al. - 2015 - Numba a LLVM-based Python JIT compiler.pdf}
}

@article{landinNext700Programming1966,
  title = {The next 700 Programming Languages},
  author = {Landin, P. J.},
  date = {1966-03-01},
  journaltitle = {Commun. ACM},
  volume = {9},
  number = {3},
  pages = {157--166},
  issn = {0001-0782},
  doi = {10.1145/365230.365257},
  url = {https://dl.acm.org/doi/10.1145/365230.365257},
  urldate = {2025-05-10},
  abstract = {A family of unimplemented computing languages is described that is intended to span differences of application area by a unified framework. This framework dictates the rules about the uses of user-coined names, and the conventions about characterizing functional relationships. Within this framework the design of a specific language splits into two independent parts. One is the choice of written appearances of programs (or more generally, their physical representation). The other is the choice of the abstract entities (such as numbers, character-strings, list of them, functional relations among them) that can be referred to in the language.The system is biased towards “expressions” rather than “statements.” It includes a nonprocedural (purely functional) subsystem that aims to expand the class of users' needs that can be met by a single print-instruction, without sacrificing the important properties that make conventional right-hand-side expressions easy to construct and understand.},
  file = {/Users/edjg/Zotero/storage/ZYESTFPK/Landin - 1966 - The next 700 programming languages.pdf}
}

@inproceedings{lattnerLLVMCompilationFramework2004,
  title = {{{LLVM}}: A Compilation Framework for Lifelong Program Analysis \& Transformation},
  shorttitle = {{{LLVM}}},
  booktitle = {International {{Symposium}} on {{Code Generation}} and {{Optimization}}, 2004. {{CGO}} 2004.},
  author = {Lattner, C. and Adve, V.},
  date = {2004-03},
  pages = {75--86},
  doi = {10.1109/CGO.2004.1281665},
  url = {https://ieeexplore.ieee.org/abstract/document/1281665},
  urldate = {2024-11-18},
  abstract = {We describe LLVM (low level virtual machine), a compiler framework designed to support transparent, lifelong program analysis and transformation for arbitrary programs, by providing high-level information to compiler transformations at compile-time, link-time, run-time, and in idle time between runs. LLVM defines a common, low-level code representation in static single assignment (SSA) form, with several novel features: a simple, language-independent type-system that exposes the primitives commonly used to implement high-level language features; an instruction for typed address arithmetic; and a simple mechanism that can be used to implement the exception handling features of high-level languages (and setjmp/longjmp in C) uniformly and efficiently. The LLVM compiler framework and code representation together provide a combination of key capabilities that are important for practical, lifelong analysis and transformation of programs. To our knowledge, no existing compilation approach provides all these capabilities. We describe the design of the LLVM representation and compiler framework, and evaluate the design in three ways: (a) the size and effectiveness of the representation, including the type information it provides; (b) compiler performance for several interprocedural problems; and (c) illustrative examples of the benefits LLVM provides for several challenging compiler problems.},
  eventtitle = {International {{Symposium}} on {{Code Generation}} and {{Optimization}}, 2004. {{CGO}} 2004.},
  keywords = {Algorithm design and analysis,Application software,Arithmetic,High level languages,Information analysis,Performance analysis,Program processors,Runtime,Software safety,Virtual machining},
  file = {/Users/edjg/Zotero/storage/KPM23KWZ/Lattner and Adve - 2004 - LLVM a compilation framework for lifelong program analysis & transformation.pdf;/Users/edjg/Zotero/storage/W7FDUUG6/1281665.html}
}

@software{lattnerLlvmLlvmproject2025,
  title = {Llvm/Llvm-Project},
  author = {Lattner, Chris},
  date = {2025-04-11T13:00:48Z},
  origdate = {2016-12-07T09:39:33Z},
  url = {https://github.com/llvm/llvm-project},
  urldate = {2025-04-11},
  abstract = {The LLVM Project is a collection of modular and reusable compiler and toolchain technologies.},
  organization = {LLVM}
}

@inproceedings{lattnerMLIRScalingCompiler2021a,
  title = {{{MLIR}}: {{Scaling Compiler Infrastructure}} for {{Domain Specific Computation}}},
  shorttitle = {{{MLIR}}},
  booktitle = {2021 {{IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}} ({{CGO}})},
  author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  date = {2021-02},
  pages = {2--14},
  doi = {10.1109/CGO51591.2021.9370308},
  url = {https://ieeexplore.ieee.org/abstract/document/9370308},
  urldate = {2025-04-11},
  abstract = {This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR addresses software fragmentation, compilation for heterogeneous hardware, significantly reducing the cost of building domain specific compilers, and connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, while identifying the challenges and opportunities posed by this novel design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.},
  eventtitle = {2021 {{IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}} ({{CGO}})},
  keywords = {Buildings,Generators,Hardware,Optimization,Program processors,Semantics,Software},
  file = {/Users/edjg/Zotero/storage/GW75VWA5/Lattner et al. - 2021 - MLIR Scaling Compiler Infrastructure for Domain Specific Computation.pdf}
}

@inproceedings{lionInvestigatingManagedLanguage2022,
  title = {Investigating {{Managed Language Runtime Performance}}: {{Why}} \{\vphantom\}{{JavaScript}}\vphantom\{\} and {{Python}} Are 8x and 29x Slower than {{C}}++, yet {{Java}} and {{Go}} Can Be {{Faster}}?},
  shorttitle = {Investigating {{Managed Language Runtime Performance}}},
  author = {Lion, David and Chiu, Adrian and Stumm, Michael and Yuan, Ding},
  date = {2022},
  pages = {835--852},
  url = {https://www.usenix.org/conference/atc22/presentation/lion},
  urldate = {2025-05-11},
  eventtitle = {2022 {{USENIX Annual Technical Conference}} ({{USENIX ATC}} 22)},
  langid = {english},
  file = {/Users/edjg/Zotero/storage/P3UA85XD/Lion et al. - 2022 - Investigating Managed Language Runtime Performance Why JavaScript and Python are 8x and 29x slowe.pdf}
}

@software{michaeldroettboomAirspeedvelocityAsv2025,
  title = {Airspeed-Velocity/Asv},
  author = {{Michael Droettboom} and {Pauli Virtanen}},
  date = {2025-04-27T07:11:05Z},
  origdate = {2013-11-07T20:43:31Z},
  url = {https://github.com/airspeed-velocity/asv},
  urldate = {2025-04-28},
  abstract = {Airspeed Velocity: A simple Python benchmarking tool with web-based reporting},
  organization = {Airspeed Velocity},
  keywords = {airspeed-velocity,benchmark,python}
}

@online{mlirteamMLIRCodeDocumentation,
  title = {{{MLIR Code Documentation}}},
  author = {{MLIR Team}},
  url = {https://mlir.llvm.org/docs/},
  urldate = {2025-04-11},
  file = {/Users/edjg/Zotero/storage/F3EKI2E6/docs.html}
}

@report{pep659,
  type = {PEP},
  title = {Specializing {{Adaptive Interpreter}}},
  author = {{Mark Shannon}},
  date = {2021},
  number = {659},
  url = {https://peps.python.org/pep-0659/}
}

@report{pep744,
  type = {PEP},
  title = {{{JIT Compilation}}},
  author = {{Brandt Bucher} and {Savannah Ostrowski}},
  date = {2024},
  number = {744},
  url = {https://peps.python.org/pep-0744/}
}

@article{saavedraPerformanceCharacterizationOptimizing1995,
  title = {Performance Characterization of Optimizing Compilers},
  author = {Saavedra, R.H. and Smith, A.J.},
  date = {1995-07},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {21},
  number = {7},
  pages = {615--628},
  issn = {1939-3520},
  doi = {10.1109/32.392982},
  url = {https://ieeexplore.ieee.org/document/392982/},
  urldate = {2025-04-24},
  abstract = {Optimizing compilers have become an essential component in achieving high levels of performance. Various simple and sophisticated optimizations are implemented at different stages of compilation to yield significant improvements, but little work has been done in characterizing the effectiveness of optimizers, or in understanding where most of this improvement comes from. We study the performance impact of optimization in the context of our methodology for CPU performance characterization based on the abstract machine model. The model considers all machines to be different implementations of the same high level language abstract machine; in previous research, the model has been used as a basis to analyze machine and benchmark performance. We show that our model can be extended to characterize the performance improvement provided by optimizers and to predict the run time of optimized programs, and measure the effectiveness of several compilers in implementing different optimization techniques.{$<>$}},
  keywords = {Application software,Central Processing Unit,Computer aided manufacturing,Computer science,Context modeling,High level languages,Optimizing compilers,Performance analysis,Predictive models,Time measurement},
  file = {/Users/edjg/Zotero/storage/7CTJVHTF/Saavedra and Smith - 1995 - Performance characterization of optimizing compilers.pdf}
}

@article{sabne2020xla,
  title = {{{XLA}}: {{Compiling}} Machine Learning for Peak Performance},
  author = {Sabne, Amit},
  date = {2020},
  journaltitle = {Google Research},
  url = {https://openxla.org/}
}

@thesis{saeed2008systematic,
  type = {phdthesis},
  title = {Systematic Review of Verification \& Validation in Dynamic Languages},
  author = {family=Saeed, given=FSM, given-i=FSM and Saeed, F},
  date = {2008},
  institution = {MS. Thesis, Blekinge Institute of Technology, Sweden},
  file = {/Users/edjg/Zotero/storage/AUBRPYLN/Saeed and Saeed - 2008 - Systematic review of verification & validation in dynamic languages.pdf}
}

@inproceedings{steeleDebunkingExpensiveProcedure1977,
  title = {Debunking the “Expensive Procedure Call” Myth or, Procedure Call Implementations Considered Harmful or, {{LAMBDA}}: {{The Ultimate GOTO}}},
  shorttitle = {Debunking the “Expensive Procedure Call” Myth or, Procedure Call Implementations Considered Harmful or, {{LAMBDA}}},
  booktitle = {Proceedings of the 1977 Annual Conference},
  author = {Steele, Guy Lewis},
  date = {1977-01-01},
  series = {{{ACM}} '77},
  pages = {153--162},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/800179.810196},
  url = {https://dl.acm.org/doi/10.1145/800179.810196},
  urldate = {2025-05-13},
  abstract = {Folklore states that GOTO statements are “cheap”, while procedure calls are “expensive”. This myth is largely a result of poorly designed language implementations. The historical growth of this myth is considered. Both theoretical ideas and an existing implementation are discussed which debunk this myth. It is shown that the unrestricted use of procedure calls permits great stylistic freedom. In particular, any flowchart can be written as a “structured” program without introducing extra variables. The difficulty with the GOTO statement and the procedure call is characterized as a conflict between abstract programming concepts and concrete language constructs.},
  isbn = {978-1-4503-3921-6},
  file = {/Users/edjg/Zotero/storage/CDD4PU2J/Steele - 1977 - Debunking the “expensive procedure call” myth or, procedure call implementations considered harmful.pdf}
}

@inproceedings{stoltzConstantPropagationFresh1994,
  title = {Constant Propagation: A Fresh, Demand-Driven Look},
  shorttitle = {Constant Propagation},
  booktitle = {Proceedings of the 1994 {{ACM}} Symposium on {{Applied}} Computing},
  author = {Stoltz, Eric and Wolfe, Michael and Gerlek, Michael P.},
  date = {1994-04-06},
  series = {{{SAC}} '94},
  pages = {400--404},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/326619.326791},
  url = {https://dl.acm.org/doi/10.1145/326619.326791},
  urldate = {2025-05-16},
  isbn = {978-0-89791-647-9},
  file = {/Users/edjg/Zotero/storage/6WAK9W8U/Stoltz et al. - 1994 - Constant propagation a fresh, demand-driven look.pdf}
}

@online{tomdaleAdventuresMicrobenchmarking2017,
  title = {Adventures in {{Microbenchmarking}}},
  author = {{Tom Dale}},
  date = {2017-07-15},
  url = {https://tomdale.net/2017/07/adventures-in-microbenchmarking/},
  urldate = {2025-04-24},
  organization = {tomdale.net},
  file = {/Users/edjg/Zotero/storage/LRBUXXIR/adventures-in-microbenchmarking.html}
}

@software{vaivaswathanagarajVaivaswathaPliron2025,
  title = {Vaivaswatha/Pliron},
  author = {{Vaivaswatha Nagaraj}},
  date = {2025-05-16T23:51:15Z},
  origdate = {2022-08-04T14:03:33Z},
  url = {https://github.com/vaivaswatha/pliron},
  urldate = {2025-05-17},
  abstract = {An Extensible Compiler IR Framework},
  keywords = {compilers,ir,mlir,programming-languages}
}

@online{victorstinnerMyJourneyStable2016,
  title = {My Journey to Stable Benchmark, Part 1 (System)},
  author = {{Victor Stinner}},
  date = {2016-05-21},
  url = {https://vstinner.github.io/journey-to-stable-benchmark-system.html},
  urldate = {2025-05-14},
  file = {/Users/edjg/Zotero/storage/YMUXNYS7/journey-to-stable-benchmark-system.html}
}

@online{victorstinnerMyJourneyStable2016a,
  title = {My Journey to Stable Benchmark, Part 2 (Deadcode)},
  author = {{Victor Stinner}},
  date = {2016-05-22},
  url = {https://vstinner.github.io/journey-to-stable-benchmark-deadcode.html},
  urldate = {2025-05-14},
  file = {/Users/edjg/Zotero/storage/KLNBNB74/journey-to-stable-benchmark-deadcode.html}
}

@online{victorstinnerMyJourneyStable2016b,
  title = {My Journey to Stable Benchmark, Part 3 (Average)},
  author = {{Victor Stinner}},
  date = {2016-05-23},
  url = {https://vstinner.github.io/journey-to-stable-benchmark-average.html},
  urldate = {2025-05-14},
  file = {/Users/edjg/Zotero/storage/QLH6WL6M/journey-to-stable-benchmark-average.html}
}

@software{victorstinnerPsfPyperf2025,
  title = {Psf/Pyperf},
  author = {{Victor Stinner}},
  date = {2025-05-12T15:01:45Z},
  origdate = {2016-06-01T13:25:17Z},
  url = {https://github.com/psf/pyperf},
  urldate = {2025-05-15},
  abstract = {Toolkit to run Python benchmarks},
  organization = {Python Software Foundation},
  keywords = {benchmarking,python}
}

@online{whatsNewPython311,
  title = {What’s {{New In Python}} 3.11},
  author = {{Pablo Galindo Salgado}},
  url = {https://docs.python.org/3/whatsnew/3.11.html},
  urldate = {2025-05-10},
  abstract = {Editor, Pablo Galindo Salgado. This article explains the new features in Python 3.11, compared to 3.10. Python 3.11 was released on October 24, 2022.},
  langid = {english},
  organization = {Python documentation},
  file = {/Users/edjg/Zotero/storage/MCFX7GTA/3.11.html}
}

@online{whatsNewPython312,
  title = {What’s {{New In Python}} 3.12},
  author = {{Adam Turner}},
  url = {https://docs.python.org/3/whatsnew/3.12.html},
  urldate = {2025-05-10},
  abstract = {Editor, Adam Turner,. This article explains the new features in Python 3.12, compared to 3.11. Python 3.12 was released on October 2, 2023. For full details, see the changelog. Summary – Release hi...},
  langid = {english},
  organization = {Python documentation},
  file = {/Users/edjg/Zotero/storage/CC9F2RGA/3.12.html}
}

@online{whatsNewPython313,
  title = {What’s {{New In Python}} 3.13},
  author = {{Adam Turner} and {Thomas Wouters}},
  url = {https://docs.python.org/3/whatsnew/3.13.html},
  urldate = {2025-05-10},
  abstract = {Editors, Adam Turner and Thomas Wouters. This article explains the new features in Python 3.13, compared to 3.12. Python 3.13 was released on October 7, 2024.},
  langid = {english},
  organization = {Python documentation},
  file = {/Users/edjg/Zotero/storage/A2824J8Z/3.13.html}
}

@inproceedings{williamsDynamicInterpretationDynamic2010,
  title = {Dynamic Interpretation for Dynamic Scripting Languages},
  booktitle = {Proceedings of the 8th Annual {{IEEE}}/{{ACM}} International Symposium on {{Code}} Generation and Optimization},
  author = {Williams, Kevin and McCandless, Jason and Gregg, David},
  date = {2010-04-24},
  series = {{{CGO}} '10},
  pages = {278--287},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1772954.1772993},
  url = {https://dl.acm.org/doi/10.1145/1772954.1772993},
  urldate = {2025-05-10},
  abstract = {Dynamic scripting languages offer programmers increased flexibility by allowing properties of programs to be defined at run-time. Typically, program execution begins with an interpreter where type checks are implemented using conditional statements. Recent JIT compilers have begun removing run-time checks by specializing native code to program properties discovered at JIT time.This paper presents a novel intermediate representation for scripting languages that explicitly encodes types of variables. The dynamic representation is a flow graph, where each node is a specialized virtual instruction and each edge directs program flow based on control and type changes in the program. The interpreter thus performs specialized execution of whole programs. We present techniques for the efficient interpretation of our representation showing speed-ups of greater than 2x over static interpretation, with an average speed-up of approximately 1.3x.},
  isbn = {978-1-60558-635-9},
  file = {/Users/edjg/Zotero/storage/FW38FJTP/Williams et al. - 2010 - Dynamic interpretation for dynamic scripting languages.pdf}
}

@online{xuBuildingBaselineJIT2023a,
  title = {Building a Baseline {{JIT}} for {{Lua}} Automatically},
  author = {Xu, Haoran},
  date = {2023-05-12T00:00:00},
  url = {https://sillycross.github.io/2023/05/12/2023-05-12/index.html},
  urldate = {2025-05-14},
  abstract = {This is the Part 2 of a series. Feel free to read the prequel for more context: Building the fastest Lua interpreter automatically  Building a good VM for a dynamic language takes a ton of engineerin},
  file = {/Users/edjg/Zotero/storage/SUVYS5UP/2023-05-12.html}
}

@online{xuBuildingFastestLua2022a,
  title = {Building the Fastest {{Lua}} Interpreter.. Automatically!},
  author = {Xu, Haoran},
  date = {2022-11-22T00:00:00},
  url = {https://sillycross.github.io/2022/11/22/2022-11-22/index.html},
  urldate = {2025-05-14},
  abstract = {This is Part 1 of a series of posts. Part 2 is available here: Building a baseline JIT for Lua automatically  It is well-known that writing a good VM for a dynamic language is never an easy job. High},
  file = {/Users/edjg/Zotero/storage/L5FXCLMQ/2022-11-22.html}
}
