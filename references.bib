@book{alfred2007compilers,
  title = {Compilers: {{Principles}}, {{Techniques}} \& {{Tools}}},
  author = {Alfred, V Aho and Monica, S Lam and Jeffrey, D Ullman},
  date = {2007},
  edition = {2},
  publisher = {pearson Education}
}

@inproceedings{amdahlValiditySingleProcessor1967,
  title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
  booktitle = {Proceedings of the {{April}} 18-20, 1967, Spring Joint Computer Conference},
  author = {Amdahl, Gene M.},
  date = {1967-04-18},
  series = {{{AFIPS}} '67 ({{Spring}})},
  pages = {483--485},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1465482.1465560},
  url = {https://dl.acm.org/doi/10.1145/1465482.1465560},
  urldate = {2025-01-13},
  abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.},
  isbn = {978-1-4503-7895-6},
  file = {/Users/edjg/Zotero/storage/VRNQ8ZCL/Amdahl - 1967 - Validity of the single processor approach to achieving large scale computing capabilities.pdf}
}

@unpublished{aminiHowSlowMLIR2024,
  title = {How {{Slow}} Is {{MLIR}}?},
  author = {Amini, Mehdi and Nui, Jeff},
  date = {2024-04},
  url = {https://www.youtube.com/watch?v=7qvVMUSxqz4},
  eventtitle = {2024 {{European LLVM Developers}}' {{Meeting}}},
  venue = {Vienna, Austria}
}

@article{aycockBriefHistoryJustintime2003,
  title = {A Brief History of Just-in-Time},
  author = {Aycock, John},
  date = {2003-06-01},
  journaltitle = {ACM Comput. Surv.},
  volume = {35},
  number = {2},
  pages = {97--113},
  issn = {0360-0300},
  doi = {10.1145/857076.857077},
  url = {https://dl.acm.org/doi/10.1145/857076.857077},
  urldate = {2025-05-11},
  abstract = {Software systems have been using "just-in-time" compilation (JIT) techniques since the 1960s. Broadly, JIT compilation includes any translation performed dynamically, after a program has started execution. We examine the motivation behind JIT compilation and constraints imposed on JIT compilation systems, and present a classification scheme for such systems. This classification emerges as we survey forty years of JIT work, from 1960--2000.},
  file = {/Users/edjg/Zotero/storage/KFW9CF8P/Aycock - 2003 - A brief history of just-in-time.pdf}
}

@inproceedings{baconFastStaticAnalysis1996,
  title = {Fast Static Analysis of {{C}}++ Virtual Function Calls},
  booktitle = {Proceedings of the 11th {{ACM SIGPLAN}} Conference on {{Object-oriented}} Programming, Systems, Languages, and Applications},
  author = {Bacon, David F. and Sweeney, Peter F.},
  date = {1996-10-01},
  series = {{{OOPSLA}} '96},
  pages = {324--341},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/236337.236371},
  url = {https://dl.acm.org/doi/10.1145/236337.236371},
  urldate = {2025-05-11},
  abstract = {Virtual functions make code easier for programmers to reuse but also make it harder for compilers to analyze. We investigate the ability of three static analysis algorithms to improve C++ programs by resolving virtual function calls, thereby reducing compiled code size and reducing program complexity so as to improve both human and automated program understanding and analysis. In measurements of seven programs of significant size (5000 to 20000 lines of code each) we found that on average the most precise of the three algorithms resolved 71\% of the virtual function calls and reduced compiled code size by 25\%. This algorithm is very fast: it analyzes 3300 source lines per second on an 80 MHz PowerPC 601. Because of its accuracy and speed, this algorithm is an excellent candidate for inclusion in production C++ compilers.},
  isbn = {978-0-89791-788-9},
  file = {/Users/edjg/Zotero/storage/LL8LCXSD/Bacon and Sweeney - 1996 - Fast static analysis of C++ virtual function calls.pdf}
}

@incollection{chatleyNext7000Programming2019,
  title = {The {{Next}} 7000 {{Programming Languages}}},
  booktitle = {Computing and {{Software Science}}: {{State}} of the {{Art}} and {{Perspectives}}},
  author = {Chatley, Robert and Donaldson, Alastair and Mycroft, Alan},
  editor = {Steffen, Bernhard and Woeginger, Gerhard},
  date = {2019},
  pages = {250--282},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-91908-9_15},
  url = {https://doi.org/10.1007/978-3-319-91908-9_15},
  urldate = {2025-05-10},
  abstract = {Landin’s seminal paper “The next 700 programming languages” considered programming languages prior to 1966 and speculated on the next 700. Half-a-century on, we cast programming languages in a Darwinian ‘tree of life’ and explore languages, their features (genes) and language evolution from the viewpoint of ‘survival of the fittest’.},
  isbn = {978-3-319-91908-9},
  langid = {english},
  file = {/Users/edjg/Zotero/storage/8ZWWCKTL/Chatley et al. - 2019 - The Next 7000 Programming Languages.pdf}
}

@inproceedings{fehrIRDLIRDefinition2022a,
  title = {{{IRDL}}: An {{IR}} Definition Language for {{SSA}} Compilers},
  shorttitle = {{{IRDL}}},
  booktitle = {Proceedings of the 43rd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Fehr, Mathieu and Niu, Jeff and Riddle, River and Amini, Mehdi and Su, Zhendong and Grosser, Tobias},
  date = {2022-06-09},
  series = {{{PLDI}} 2022},
  pages = {199--212},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3519939.3523700},
  url = {https://dl.acm.org/doi/10.1145/3519939.3523700},
  urldate = {2025-04-11},
  abstract = {Designing compiler intermediate representations (IRs) is often a manual process that makes exploration and innovation in this space costly. Developers typically use general-purpose programming languages to design IRs. As a result, IR implementations are verbose, manual modifications are expensive, and designing tooling for the inspection or generation of IRs is impractical. While compilers relied historically on a few slowly evolving IRs, domain-specific optimizations and specialized hardware motivate compilers to use and evolve many IRs. We facilitate the implementation of SSA-based IRs by introducing IRDL, a domain-specific language to define IRs. We analyze all 28 domain-specific IRs developed as part of LLVM's MLIR project over the last two years and demonstrate how to express these IRs exclusively in IRDL while only rarely falling back to IRDL's support for generic C++ extensions. By enabling the concise and explicit specification of IRs, we provide foundations for developing effective tooling to automate the compiler construction process.},
  isbn = {978-1-4503-9265-5},
  file = {/Users/edjg/Zotero/storage/VB4Z3UNB/Fehr et al. - 2022 - IRDL an IR definition language for SSA compilers.pdf}
}

@software{fehrXdslprojectXdsl2024,
  title = {Xdslproject/Xdsl},
  author = {Fehr, Mathieu and Lopoukhine, Alexandre},
  date = {2024-10-21T13:01:34Z},
  origdate = {2021-09-12T14:02:30Z},
  url = {https://github.com/xdslproject/xdsl},
  urldate = {2024-10-21},
  abstract = {A Python Compiler Design Toolkit},
  organization = {xdslproject}
}

@inproceedings{fehrXDSLSidekickCompilation2025,
  title = {{{xDSL}}: {{Sidekick Compilation}} for {{SSA-Based Compilers}}},
  shorttitle = {{{xDSL}}},
  booktitle = {Proceedings of the 23rd {{ACM}}/{{IEEE International Symposium}} on {{Code Generation}} and {{Optimization}}},
  author = {Fehr, Mathieu and Weber, Michel and Ulmann, Christian and Lopoukhine, Alexandre and Lücke, Martin Paul and Degioanni, Théo and Vasiladiotis, Christos and Steuwer, Michel and Grosser, Tobias},
  date = {2025-03-01},
  series = {{{CGO}} '25},
  pages = {179--192},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3696443.3708945},
  url = {https://dl.acm.org/doi/10.1145/3696443.3708945},
  urldate = {2025-04-11},
  abstract = {Traditionally, compiler researchers either conduct experiments within an existing production compiler or develop their own prototype compiler; both options come with trade-offs.                                On one hand, prototyping in a production compiler can be cumbersome, as they are often optimized for program compilation speed at the expense of software simplicity and development speed.                                On the other hand, the transition from a prototype compiler to production requires significant engineering work.                                To bridge this gap, we introduce the concept of sidekick compiler frameworks, an approach that uses multiple frameworks that interoperate with each other by leveraging textual interchange formats and declarative descriptions of abstractions.                                Each such compiler framework is specialized for specific use cases, such as performance or prototyping.                                Abstractions are by design shared across frameworks, simplifying the transition from prototyping to production.                                We demonstrate this idea with xDSL, a sidekick for MLIR focused on prototyping and teaching.                                xDSL interoperates with MLIR through a shared textual IR and the exchange of IRs through an IR Definition Language.                                The benefits of sidekick compiler frameworks are evaluated by showing on three use cases how xDSL impacts their development: teaching, DSL compilation, and rewrite system prototyping.                                We also investigate the trade-offs that xDSL offers, and demonstrate how we simplify the transition between frameworks using the IRDL dialect.                                With sidekick compilation, we envision a future in which engineers minimize the cost of development by choosing a framework built for their immediate needs, and later transitioning to production with minimal overhead.},
  isbn = {979-8-4007-1275-3},
  file = {/Users/edjg/Zotero/storage/7YEB29W8/Fehr et al. - 2025 - xDSL Sidekick Compilation for SSA-Based Compilers.pdf}
}

@software{gaoVizTracer2025,
  title = {{{VizTracer}}},
  author = {Gao, Tian},
  date = {2025-04-28T14:34:17Z},
  origdate = {2020-08-05T00:29:56Z},
  url = {https://github.com/gaogaotiantian/viztracer},
  urldate = {2025-04-28},
  abstract = {A debugging and profiling tool that can trace and visualize python code execution}
}

@software{guidovanrossumPythonCpython2025,
  title = {Python/Cpython},
  author = {{Guido van Rossum}},
  date = {2025-04-28T14:44:57Z},
  origdate = {2017-02-10T19:23:51Z},
  url = {https://github.com/python/cpython},
  urldate = {2025-04-28},
  abstract = {The Python programming language},
  organization = {Python}
}

@inproceedings{holzleOptimizingDynamicallydispatchedCalls1994,
  title = {Optimizing Dynamically-Dispatched Calls with Run-Time Type Feedback},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 1994 Conference on {{Programming}} Language Design and Implementation},
  author = {Hölzle, Urs and Ungar, David},
  date = {1994-06-01},
  series = {{{PLDI}} '94},
  pages = {326--336},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/178243.178478},
  url = {https://dl.acm.org/doi/10.1145/178243.178478},
  urldate = {2025-05-12},
  isbn = {978-0-89791-662-2},
  file = {/Users/edjg/Zotero/storage/4AMQL6SQ/Hölzle and Ungar - 1994 - Optimizing dynamically-dispatched calls with run-time type feedback.pdf}
}

@online{kempenItsNotEasy2025,
  title = {It's {{Not Easy Being Green}}: {{On}} the {{Energy Efficiency}} of {{Programming Languages}}},
  shorttitle = {It's {{Not Easy Being Green}}},
  author = {family=Kempen, given=Nicolas, prefix=van, useprefix=false and Kwon, Hyuk-Je and Nguyen, Dung Tuan and Berger, Emery D.},
  date = {2025-03-28},
  eprint = {2410.05460},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.05460},
  url = {http://arxiv.org/abs/2410.05460},
  urldate = {2025-05-11},
  abstract = {Does the choice of programming language affect energy consumption? Previous highly visible studies have established associations between certain programming languages and energy consumption. A causal misinterpretation of this work has led academics and industry leaders to use or support certain languages based on their claimed impact on energy consumption. This paper tackles this causal question directly. It first corrects and improves the measurement methodology used by prior work. It then develops a detailed causal model capturing the complex relationship between programming language choice and energy consumption. This model identifies and incorporates several critical but previously overlooked factors that affect energy usage. These factors, such as distinguishing programming languages from their implementations, the impact of the application implementations themselves, the number of active cores, and memory activity, can significantly skew energy consumption measurements if not accounted for. We show -- via empirical experiments, improved methodology, and careful examination of anomalies -- that when these factors are controlled for, notable discrepancies in prior work vanish. Our analysis suggests that the choice of programming language implementation has no significant impact on energy consumption beyond execution time.},
  pubstate = {prepublished},
  keywords = {Computer Science - Performance,Computer Science - Programming Languages},
  file = {/Users/edjg/Zotero/storage/C65BSPZP/Kempen et al. - 2025 - It's Not Easy Being Green On the Energy Efficiency of Programming Languages.pdf;/Users/edjg/Zotero/storage/WLEKD4IA/2410.html}
}

@inproceedings{laaberEvaluationOpensourceSoftware2018,
  title = {An Evaluation of Open-Source Software Microbenchmark Suites for Continuous Performance Assessment},
  booktitle = {Proceedings of the 15th {{International Conference}} on {{Mining Software Repositories}}},
  author = {Laaber, Christoph and Leitner, Philipp},
  date = {2018-05-28},
  series = {{{MSR}} '18},
  pages = {119--130},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3196398.3196407},
  url = {https://dl.acm.org/doi/10.1145/3196398.3196407},
  urldate = {2025-04-24},
  abstract = {Continuous integration (CI) emphasizes quick feedback to developers. This is at odds with current practice of performance testing, which predominantely focuses on long-running tests against entire systems in production-like environments. Alternatively, software microbenchmarking attempts to establish a performance baseline for small code fragments in short time. This paper investigates the quality of microbenchmark suites with a focus on suitability to deliver quick performance feedback and CI integration. We study ten open-source libraries written in Java and Go with benchmark suite sizes ranging from 16 to 983 tests, and runtimes between 11 minutes and 8.75 hours. We show that our study subjects include benchmarks with result variability of 50\% or higher, indicating that not all benchmarks are useful for reliable discovery of slowdowns. We further artificially inject actual slowdowns into public API methods of the study subjects and test whether test suites are able to discover them. We introduce a performance-test quality metric called the API benchmarking score (ABS). ABS represents a benchmark suite's ability to find slowdowns among a set of defined core API methods. Resulting benchmarking scores (i.e., fraction of discovered slowdowns) vary between 10\% and 100\% for the study subjects. This paper's methodology and results can be used to (1) assess the quality of existing microbenchmark suites, (2) select a set of tests to be run as part of CI, and (3) suggest or generate benchmarks for currently untested parts of an API.},
  isbn = {978-1-4503-5716-6},
  file = {/Users/edjg/Zotero/storage/8MD74V8C/Laaber and Leitner - 2018 - An evaluation of open-source software microbenchmark suites for continuous performance assessment.pdf}
}

@inproceedings{lamNumbaLLVMbasedPython2015,
  title = {Numba: A {{LLVM-based Python JIT}} Compiler},
  shorttitle = {Numba},
  booktitle = {Proceedings of the {{Second Workshop}} on the {{LLVM Compiler Infrastructure}} in {{HPC}}},
  author = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
  date = {2015-11-15},
  series = {{{LLVM}} '15},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2833157.2833162},
  url = {https://dl.acm.org/doi/10.1145/2833157.2833162},
  urldate = {2025-05-11},
  abstract = {Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is often a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addition, we share our experience in building a JIT compiler using LLVM[1].},
  isbn = {978-1-4503-4005-2},
  file = {/Users/edjg/Zotero/storage/YIM4DLPJ/Lam et al. - 2015 - Numba a LLVM-based Python JIT compiler.pdf}
}

@article{landinNext700Programming1966,
  title = {The next 700 Programming Languages},
  author = {Landin, P. J.},
  date = {1966-03-01},
  journaltitle = {Commun. ACM},
  volume = {9},
  number = {3},
  pages = {157--166},
  issn = {0001-0782},
  doi = {10.1145/365230.365257},
  url = {https://dl.acm.org/doi/10.1145/365230.365257},
  urldate = {2025-05-10},
  abstract = {A family of unimplemented computing languages is described that is intended to span differences of application area by a unified framework. This framework dictates the rules about the uses of user-coined names, and the conventions about characterizing functional relationships. Within this framework the design of a specific language splits into two independent parts. One is the choice of written appearances of programs (or more generally, their physical representation). The other is the choice of the abstract entities (such as numbers, character-strings, list of them, functional relations among them) that can be referred to in the language.The system is biased towards “expressions” rather than “statements.” It includes a nonprocedural (purely functional) subsystem that aims to expand the class of users' needs that can be met by a single print-instruction, without sacrificing the important properties that make conventional right-hand-side expressions easy to construct and understand.},
  file = {/Users/edjg/Zotero/storage/ZYESTFPK/Landin - 1966 - The next 700 programming languages.pdf}
}

@inproceedings{lattnerLLVMCompilationFramework2004,
  title = {{{LLVM}}: A Compilation Framework for Lifelong Program Analysis \& Transformation},
  shorttitle = {{{LLVM}}},
  booktitle = {International {{Symposium}} on {{Code Generation}} and {{Optimization}}, 2004. {{CGO}} 2004.},
  author = {Lattner, C. and Adve, V.},
  date = {2004-03},
  pages = {75--86},
  doi = {10.1109/CGO.2004.1281665},
  url = {https://ieeexplore.ieee.org/abstract/document/1281665},
  urldate = {2024-11-18},
  abstract = {We describe LLVM (low level virtual machine), a compiler framework designed to support transparent, lifelong program analysis and transformation for arbitrary programs, by providing high-level information to compiler transformations at compile-time, link-time, run-time, and in idle time between runs. LLVM defines a common, low-level code representation in static single assignment (SSA) form, with several novel features: a simple, language-independent type-system that exposes the primitives commonly used to implement high-level language features; an instruction for typed address arithmetic; and a simple mechanism that can be used to implement the exception handling features of high-level languages (and setjmp/longjmp in C) uniformly and efficiently. The LLVM compiler framework and code representation together provide a combination of key capabilities that are important for practical, lifelong analysis and transformation of programs. To our knowledge, no existing compilation approach provides all these capabilities. We describe the design of the LLVM representation and compiler framework, and evaluate the design in three ways: (a) the size and effectiveness of the representation, including the type information it provides; (b) compiler performance for several interprocedural problems; and (c) illustrative examples of the benefits LLVM provides for several challenging compiler problems.},
  eventtitle = {International {{Symposium}} on {{Code Generation}} and {{Optimization}}, 2004. {{CGO}} 2004.},
  keywords = {Algorithm design and analysis,Application software,Arithmetic,High level languages,Information analysis,Performance analysis,Program processors,Runtime,Software safety,Virtual machining},
  file = {/Users/edjg/Zotero/storage/KPM23KWZ/Lattner and Adve - 2004 - LLVM a compilation framework for lifelong program analysis & transformation.pdf;/Users/edjg/Zotero/storage/W7FDUUG6/1281665.html}
}

@software{lattnerLlvmLlvmproject2025,
  title = {Llvm/Llvm-Project},
  author = {Lattner, Chris},
  date = {2025-04-11T13:00:48Z},
  origdate = {2016-12-07T09:39:33Z},
  url = {https://github.com/llvm/llvm-project},
  urldate = {2025-04-11},
  abstract = {The LLVM Project is a collection of modular and reusable compiler and toolchain technologies.},
  organization = {LLVM}
}

@inproceedings{lattnerMLIRScalingCompiler2021a,
  title = {{{MLIR}}: {{Scaling Compiler Infrastructure}} for {{Domain Specific Computation}}},
  shorttitle = {{{MLIR}}},
  booktitle = {2021 {{IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}} ({{CGO}})},
  author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  date = {2021-02},
  pages = {2--14},
  doi = {10.1109/CGO51591.2021.9370308},
  url = {https://ieeexplore.ieee.org/abstract/document/9370308},
  urldate = {2025-04-11},
  abstract = {This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR addresses software fragmentation, compilation for heterogeneous hardware, significantly reducing the cost of building domain specific compilers, and connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, while identifying the challenges and opportunities posed by this novel design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.},
  eventtitle = {2021 {{IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}} ({{CGO}})},
  keywords = {Buildings,Generators,Hardware,Optimization,Program processors,Semantics,Software},
  file = {/Users/edjg/Zotero/storage/GW75VWA5/Lattner et al. - 2021 - MLIR Scaling Compiler Infrastructure for Domain Specific Computation.pdf}
}

@inproceedings{lionInvestigatingManagedLanguage2022,
  title = {Investigating {{Managed Language Runtime Performance}}: {{Why}} \{\vphantom\}{{JavaScript}}\vphantom\{\} and {{Python}} Are 8x and 29x Slower than {{C}}++, yet {{Java}} and {{Go}} Can Be {{Faster}}?},
  shorttitle = {Investigating {{Managed Language Runtime Performance}}},
  author = {Lion, David and Chiu, Adrian and Stumm, Michael and Yuan, Ding},
  date = {2022},
  pages = {835--852},
  url = {https://www.usenix.org/conference/atc22/presentation/lion},
  urldate = {2025-05-11},
  eventtitle = {2022 {{USENIX Annual Technical Conference}} ({{USENIX ATC}} 22)},
  langid = {english},
  file = {/Users/edjg/Zotero/storage/P3UA85XD/Lion et al. - 2022 - Investigating Managed Language Runtime Performance Why JavaScript and Python are 8x and 29x slowe.pdf}
}

@software{michaeldroettboomAirspeedvelocityAsv2025,
  title = {Airspeed-Velocity/Asv},
  author = {{Michael Droettboom} and {Pauli Virtanen}},
  date = {2025-04-27T07:11:05Z},
  origdate = {2013-11-07T20:43:31Z},
  url = {https://github.com/airspeed-velocity/asv},
  urldate = {2025-04-28},
  abstract = {Airspeed Velocity: A simple Python benchmarking tool with web-based reporting},
  organization = {Airspeed Velocity},
  keywords = {airspeed-velocity,benchmark,python}
}

@online{mlirteamMLIRCodeDocumentation,
  title = {{{MLIR Code Documentation}}},
  author = {{MLIR Team}},
  url = {https://mlir.llvm.org/docs/},
  urldate = {2025-04-11},
  file = {/Users/edjg/Zotero/storage/F3EKI2E6/docs.html}
}

@report{pep659,
  type = {PEP},
  title = {Specializing {{Adaptive Interpreter}}},
  author = {{Mark Shannon}},
  date = {2021},
  number = {659},
  url = {https://peps.python.org/pep-0659/}
}

@report{pep744,
  type = {PEP},
  title = {{{JIT Compilation}}},
  author = {{Brandt Bucher} and {Savannah Ostrowski}},
  date = {2024},
  number = {744},
  url = {https://peps.python.org/pep-0744/}
}

@article{saavedraPerformanceCharacterizationOptimizing1995,
  title = {Performance Characterization of Optimizing Compilers},
  author = {Saavedra, R.H. and Smith, A.J.},
  date = {1995-07},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {21},
  number = {7},
  pages = {615--628},
  issn = {1939-3520},
  doi = {10.1109/32.392982},
  url = {https://ieeexplore.ieee.org/document/392982/},
  urldate = {2025-04-24},
  abstract = {Optimizing compilers have become an essential component in achieving high levels of performance. Various simple and sophisticated optimizations are implemented at different stages of compilation to yield significant improvements, but little work has been done in characterizing the effectiveness of optimizers, or in understanding where most of this improvement comes from. We study the performance impact of optimization in the context of our methodology for CPU performance characterization based on the abstract machine model. The model considers all machines to be different implementations of the same high level language abstract machine; in previous research, the model has been used as a basis to analyze machine and benchmark performance. We show that our model can be extended to characterize the performance improvement provided by optimizers and to predict the run time of optimized programs, and measure the effectiveness of several compilers in implementing different optimization techniques.{$<>$}},
  keywords = {Application software,Central Processing Unit,Computer aided manufacturing,Computer science,Context modeling,High level languages,Optimizing compilers,Performance analysis,Predictive models,Time measurement},
  file = {/Users/edjg/Zotero/storage/7CTJVHTF/Saavedra and Smith - 1995 - Performance characterization of optimizing compilers.pdf}
}

@thesis{saeed2008systematic,
  type = {phdthesis},
  title = {Systematic Review of Verification \& Validation in Dynamic Languages},
  author = {family=Saeed, given=FSM, given-i=FSM and Saeed, F},
  date = {2008},
  institution = {MS. Thesis, Blekinge Institute of Technology, Sweden},
  file = {/Users/edjg/Zotero/storage/AUBRPYLN/Saeed and Saeed - 2008 - Systematic review of verification & validation in dynamic languages.pdf}
}

@online{tomdaleAdventuresMicrobenchmarking2017,
  title = {Adventures in {{Microbenchmarking}}},
  author = {{Tom Dale}},
  date = {2017-07-15},
  url = {https://tomdale.net/2017/07/adventures-in-microbenchmarking/},
  urldate = {2025-04-24},
  organization = {tomdale.net},
  file = {/Users/edjg/Zotero/storage/LRBUXXIR/adventures-in-microbenchmarking.html}
}

@online{whatsNewPython311,
  title = {What’s {{New In Python}} 3.11},
  author = {{Pablo Galindo Salgado}},
  url = {https://docs.python.org/3/whatsnew/3.11.html},
  urldate = {2025-05-10},
  abstract = {Editor, Pablo Galindo Salgado. This article explains the new features in Python 3.11, compared to 3.10. Python 3.11 was released on October 24, 2022.},
  langid = {english},
  organization = {Python documentation},
  file = {/Users/edjg/Zotero/storage/MCFX7GTA/3.11.html}
}

@online{whatsNewPython312,
  title = {What’s {{New In Python}} 3.12},
  author = {{Adam Turner}},
  url = {https://docs.python.org/3/whatsnew/3.12.html},
  urldate = {2025-05-10},
  abstract = {Editor, Adam Turner,. This article explains the new features in Python 3.12, compared to 3.11. Python 3.12 was released on October 2, 2023. For full details, see the changelog. Summary – Release hi...},
  langid = {english},
  organization = {Python documentation},
  file = {/Users/edjg/Zotero/storage/CC9F2RGA/3.12.html}
}

@online{whatsNewPython313,
  title = {What’s {{New In Python}} 3.13},
  author = {{Adam Turner} and {Thomas Wouters}},
  url = {https://docs.python.org/3/whatsnew/3.13.html},
  urldate = {2025-05-10},
  abstract = {Editors, Adam Turner and Thomas Wouters. This article explains the new features in Python 3.13, compared to 3.12. Python 3.13 was released on October 7, 2024.},
  langid = {english},
  organization = {Python documentation},
  file = {/Users/edjg/Zotero/storage/A2824J8Z/3.13.html}
}

@inproceedings{williamsDynamicInterpretationDynamic2010,
  title = {Dynamic Interpretation for Dynamic Scripting Languages},
  booktitle = {Proceedings of the 8th Annual {{IEEE}}/{{ACM}} International Symposium on {{Code}} Generation and Optimization},
  author = {Williams, Kevin and McCandless, Jason and Gregg, David},
  date = {2010-04-24},
  series = {{{CGO}} '10},
  pages = {278--287},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1772954.1772993},
  url = {https://dl.acm.org/doi/10.1145/1772954.1772993},
  urldate = {2025-05-10},
  abstract = {Dynamic scripting languages offer programmers increased flexibility by allowing properties of programs to be defined at run-time. Typically, program execution begins with an interpreter where type checks are implemented using conditional statements. Recent JIT compilers have begun removing run-time checks by specializing native code to program properties discovered at JIT time.This paper presents a novel intermediate representation for scripting languages that explicitly encodes types of variables. The dynamic representation is a flow graph, where each node is a specialized virtual instruction and each edge directs program flow based on control and type changes in the program. The interpreter thus performs specialized execution of whole programs. We present techniques for the efficient interpretation of our representation showing speed-ups of greater than 2x over static interpretation, with an average speed-up of approximately 1.3x.},
  isbn = {978-1-60558-635-9},
  file = {/Users/edjg/Zotero/storage/FW38FJTP/Williams et al. - 2010 - Dynamic interpretation for dynamic scripting languages.pdf}
}
