\chapter{Related work}
\label{chap:related-work}

% This chapter covers relevant (and typically, recent) research
% which you build upon (or improve upon). There are two complementary
% goals for this chapter:
% \begin{enumerate}
%   \item to show that you know and understand the state of the art; and
%   \item to put your work in context
% \end{enumerate}
%
% Ideally you can tackle both together by providing a critique of
% related work, and describing what is insufficient (and how you do
% better!)
%
% The related work chapter should usually come either near the front or
% near the back of the dissertation. The advantage of the former is that
% you get to build the argument for why your work is important before
% presenting your solution(s) in later chapters; the advantage of the
% latter is that don't have to forward reference to your solution too
% much. The correct choice will depend on what you're writing up, and
% your own personal preference.

%% Introduction
% Hook
% TODO: Think of a snazzy topic sentence
% Argument
In this section, we discuss existing bodies of work underpinning our thesis, and identify research gaps which our contributions aim to fill.
% Link


\section{Performance of user-extensible compiler frameworks}
\label{sec:perf-user-extensible-frameworks}

%% General vibe of the community?
% Hook
While compiler designers naturally focus on optimizing the performance of compiled code, the execution time of the compiler itself (henceforth referred to as compiler performance) also has a significant impact on developer productivity.
% Argument
For very large projects such as Firefox, which contains over twenty million lines of code \cite{bastienabadieEngineeringCodeQuality}, small changes to compiler performance can result in minutes gained or lost for each compilation -- significantly impacting developer productivity. Although there has been considerable engineering effort devoted to compiler performance, academic research continues to focus primarily on improving the compiled output rather than the compilation process itself.
% Link
As such, researchers have not yet fully explored the field, with only a few academic studies examining the performance of compiler frameworks, the most salient of which we discuss below.

%% Performance of LLVM
% Hook
Lattner and Adve's original paper proposing LLVM contains a short evaluation of the framework's performance \cite[Section 4.1.4]{lattnerLLVMCompilationFramework2004}.
% Argument
In this evaluation, the authors compare the runtime of individual transformation passes against \texttt{gcc} optimisation level \texttt{-O3} across a variety of workloads.
The results of this experiment \cite[Table 2]{lattnerLLVMCompilationFramework2004} report each of LLVM's transformation passes is at least two orders of magnitude faster than \texttt{gcc}'s end-to-end compilation across the tested workloads. This demonstrates that analysis and transformations can be performed efficiently, but has a critical flaw. By using end-to-end compilation time for \texttt{gcc}, the measurement includes time taken by non-transformation phases such as parsing, printing, and code generation, making it incomparable with the measurements of the transformation passes only for LLVM.
Furthermore, since the paper's publication twenty years ago, LLVM has evolved significantly. This evolution brings new complexity, new performance enhancements, and even new frameworks such as \ac{mlir} -- changing the calculus of its performance characteristics.
% Link
This motivated later research to re-examine these performance characteristics of compiler frameworks.

%% How slow is MLIR?
% Hook
At the 2024 European LLVM Developers' Meeting, Mehdi Amini and Jeff Nui presented their keynote talk ``How Slow is MLIR?'' \cite{aminiHowSlowMLIR2024}.
% Argument
This talk aimed to quantify the feeling in the LLVM community that \ac{mlir} incurred a significant performance cost over LLVM alone, and produce metrics against which \ac{mlir} could be optimised.
The presenters first discuss the implementation details of \ac{mlir}, including the design choices made to match common workloads.
Next, they provide a set of micro-benchmarks for key functionality provided by \ac{mlir}, along with traditional benchmarks for constant folding and loop unrolling workloads. For the micro-benchmarks and constant folding workloads, \ac{mlir} is approximately four times slower than traditional LLVM. However, \ac{mlir}'s more expressive \ac{ir} representation yields an eighty-eight times speed up over LLVM for loop unrolling.
% Link
While these results provides valuable insights into \ac{mlir}'s performance characteristics, further research is needed to fully understand the trade-offs between expressiveness and efficiency.


%% Lack of other works/our differentiation
% Hook
Our work differentiates itself from existing research in two ways.
% Argument
Firstly, existing research focusses only on the performance characteristics of LLVM and \ac{mlir}. We extend this to also examine xDSL's performance.
Secondly, having performance measurements and instrumentation for both \ac{mlir} and xDSL, we further extend the domain by contrasting the two frameworks through the lens of dynamism and its impact on performance.
% Link






\section{Performance of the Python programming language}
\label{sec:python-performance}

%% Introduction
% Hook
Driven by Python's immense popularity, significant research effort has been expended developing tools and techniques to characterise its performance.
% Argument
% Link
This section discusses a relevant subset of these tools and techniques, and contrasts them with our novel contributions.


\subsection{Measuring application performance in Python}
\label{sec:python-performance-application}

%% Difficulty measuring python
% Hook
Reliable and accurate performance measurement is notoriously difficult.
As such, its careful execution constitutes the main contribution of systems papers and theses \cite{crapeperformance} \cite{harris2021understanding}.
% Argument
This difficulty comes from both sides of the hardware-software interface.
For example, hardware optimisations such as hierarchical caches, branch predictors, and power management schemes introduce noise, making performance measurements less predictable and consistent. Similar confounding effects come from software, from process scheduling in the operating system to garbage collection in language runtimes.
Beyond this, advanced interpreters leverage runtime performance information for adaptive specialisation and JIT compilation, further muddling measurements. This phenomenon is explored by Barrett et al.'s ``Virtual Machine Warmup Blows Hot and Cold'' \cite{barrettVirtualMachineWarmup2017}, where interpreter virtual machine warmup is shown to be highly variable, with benchmarks taking over 2000 iterations to reach a steady state.
As such, accurate measurement of the performance characteristics of a Python program is more involved than the na\"ive approach of taking the wall time it takes to execute -- requiring additional tools and techniques to guarantee reliable results.
Fortunately, Python's strong ecosystem provides a wide variety of tools to achieve this goal, from the simple standard library \texttt{timeit} utility to the \texttt{pyperf} \cite{victorstinnerPsfPyperf2025} package, with more complex control over confounding effects such as warm-ups and CPU isolation.
% Link
Our work leverages these tools to make accurate measurements of compiler framework performance.

%% Our measurements and infrastructure
% Hook
A key contribution of our research is our application of these tools to produce robust performance measurements and analysis of the xDSL user-extensible compiler frameworks, extending and contrasting similar work for \ac{mlir}.
% Argument
In addition to the research contribution of these measurements themselves, our work further supports ongoing research using the xDSL framework by providing re-usable performance benchmarks and associated tooling to measure performance.
% Link
However, sometimes measurements with finer than end-to-end granularity are required to characterise performance properties more deeply. As such, our tooling also provides a simple user interface for applying performance profilers to these benchmarks.


\subsection{Profiling to understand Python's performance}
\label{sec:python-performance-profiling}

%% Research gap
% Hook
Existing profilers for Python typically operate at the function level.
% Argument
For example, Python's standard library provides the \texttt{profile} module, a Python-native tracing profiler, along with \texttt{cProfile}, a more performant C implementation of the same functionality \cite{pythonsoftwarefoundationPythonProfilers}. These instrument each call event, providing accurate profiling information for each evaluated function.
Beyond the standard library, profilers such as \texttt{pyinstrument} use statistical sampling rather than tracing to reduce overhead incurred by performance measurement \cite{rickerbyPyinstrument2025}.
In addition to this, the recent OSDI best paper winner ``Triangulating Python Performance Issues with SCALENE'' \cite{bergerTriangulatingPythonPerformance2023a} introduces another profiler which focusses on the \ac{ffi} boundary between C and Python, a key bottleneck for the best practice of delegating computation to fast low-level implementations.
This delegation is particularly effective for structured workloads such as linear algebra, but is less suitable for highly dynamic workloads.
As such, profilers for pure Python are still important for these applications.
Furthermore, profiling information at a finer granularity than the function level is often needed to deeply the performance of a program.
% Link
\texttt{line\_profiler} provides this functionality to a line level \cite{robertkernPyutilsLine_profiler2025}, but this is still one level of abstraction over the increasingly complex implementation of CPython's interpreter.

%% ByteSight
% Hook
We fill this gap in the existing provision with ByteSight, a Python-native tracing performance profiler at the bytecode level.
% Argument
ByteSight extends existing work outputting and rewriting bytecode sequences \cite{0xecCodingReversingHacking2017} \cite{clementrouaultUnderstandingPythonExecution} \cite{nedbatchelderWickedHackPython2008}, providing an easily installable package with the novel capability of performance profiling individual bytecode instructions.
% Link
This contribution also unblocks other work in this thesis, facilitating close examination of specialised implementations and providing information about the performance of dynamic bytecode instructions.


% \section{Impact of dynamism on program performance}
% \label{}
% TODO: Lift and fill!
