\chapter{Related work}
\label{chap:related-work}

% This chapter covers relevant (and typically, recent) research
% which you build upon (or improve upon). There are two complementary
% goals for this chapter:
% \begin{enumerate}
%   \item to show that you know and understand the state of the art; and
%   \item to put your work in context
% \end{enumerate}
%
% Ideally you can tackle both together by providing a critique of
% related work, and describing what is insufficient (and how you do
% better!)
%
% The related work chapter should usually come either near the front or
% near the back of the dissertation. The advantage of the former is that
% you get to build the argument for why your work is important before
% presenting your solution(s) in later chapters; the advantage of the
% latter is that don't have to forward reference to your solution too
% much. The correct choice will depend on what you're writing up, and
% your own personal preference.

%% Introduction
% Hook
Our research builds on top of two existing bodies of work.
% Argument
The first is characterisation of the performance of user-extensible compiler frameworks. Existing work in this domain is limited to examining frameworks written in ahead-of-time compiled languages, such as \ac{mlir} in C++.
Our work extends this by measuring the performance of xDSL, a framework written in Python -- an interpreted dynamic language. This measurement then allows us to contrast the performance characteristics of static and dynamic implementations for this workload.
The second body of work is the performance measurement of the Python programming language.
% FFI boundaries and dynamism
Recent work discusses the optimisation of Python code by delegating execution to faster, compiled implementations through library calls. However, we argue that this approach is not suitable for some applications, including user-extensible compiler infrastructure.
% Applying performance measurement techniques
To substantiate this claim, we apply existing tools and techniques for measuring Python performance.
% Filling the research gap with bytecode level profiling
However, these tools can only resolve performance measurements to the line level. Our need for even finer granularity measurements to characterise the performance of dynamic workload demonstrates a gap in this existing provision. Our novel bytecode profiling tool, ByteSight, fills this gap by recording highly granular timing information for the bytecodes dispatched to interpret native Python code.
% Link
This facilitates deep performance analysis of workloads which benefit from remaining in Python natively.


\section{Performance of user-extensible compiler frameworks}
\label{sec:perf-user-extensible-frameworks}

%% General vibe of the community?
% Hook
While compiler designers naturally focus on optimizing the performance of compiled code, the execution time of the compiler itself (henceforth referred to as compiler performance) also has a significant impact on developer productivity.
% Argument
For very large projects such as Firefox, which contains over twenty million lines of code \cite{bastienabadieEngineeringCodeQuality}, small changes to compiler performance can result in minutes gained or lost for each compilation -- significantly impacting developer productivity. Although there has been considerable engineering effort devoted to compiler performance, academic research continues to focus primarily on improving the compiled output rather than the compilation process itself.
% Link
As such, researchers have not yet fully explored the field, with only a few academic studies examining the performance of compiler frameworks, the most salient of which we discuss below.

%% Performance of LLVM
% Hook
Lattner and Adve's original paper proposing LLVM contains a short evaluation of the framework's performance \cite[Section 4.1.4]{lattnerLLVMCompilationFramework2004}.
% Argument
In this evaluation, the authors compare the runtime of individual transformation passes against \texttt{gcc} optimisation level \texttt{-O3} across a variety of workloads.
The results of this experiment \cite[Table 2]{lattnerLLVMCompilationFramework2004} report each of LLVM's transformation passes is at least two orders of magnitude faster than \texttt{gcc}'s end-to-end compilation across the tested workloads. This demonstrates that analysis and transformations can be performed efficiently, but has a critical flaw. By using end-to-end compilation time for \texttt{gcc}, the measurement includes time taken by non-transformation phases such as parsing, printing, and code generation, making it incomparable with the measurements of the transformation passes only for LLVM.
Furthermore, since the paper's publication twenty years ago, LLVM has evolved significantly. This evolution brings new complexity, new performance enhancements, and even new frameworks such as \ac{mlir} -- changing the calculus of its performance characteristics.
% Link
This motivated later research to re-examine these performance characteristics of compiler frameworks.

%% How slow is MLIR?
% Hook
At the 2024 European LLVM Developers' Meeting, Mehdi Amini and Jeff Nui presented their keynote talk ``How Slow is MLIR?'' \cite{aminiHowSlowMLIR2024}.
% Argument
This talk aimed to quantify the feeling in the LLVM community that \ac{mlir} incurred a significant performance cost over LLVM alone, and produce metrics against which \ac{mlir} could be optimised.
The presenters first discuss the implementation details of \ac{mlir}, including the design choices made to match common workloads.
Next, they provide a set of micro-benchmarks for key functionality provided by \ac{mlir}, along with traditional benchmarks for constant folding and loop unrolling workloads. For the micro-benchmarks and constant folding workloads, \ac{mlir} is approximately four times slower than traditional LLVM. However, \ac{mlir}'s more expressive \ac{ir} representation yields an eighty-eight times speed up over LLVM for loop unrolling.
% Link
While these results provides valuable insights into \ac{mlir}'s performance characteristics, further research is needed to fully understand the trade-offs between expressiveness and efficiency.


%% Lack of other works/our differentiation
% Hook
Our work differentiates itself from existing research in two ways.
% Argument
Firstly, existing research focusses only on the performance characteristics of LLVM and \ac{mlir}. We extend this to also examine xDSL's performance.
Secondly, having performance measurements and instrumentation for both \ac{mlir} and xDSL, we further extend the domain by contrasting the two frameworks through the lens of dynamism and its impact on performance.
% Link






\section{Performance of the Python programming language}
\label{sec:python-performance}

%% Introduction
% Hook
Driven by Python's immense popularity, significant research effort has been expended developing tools and techniques to characterise its performance.
% Argument
% Link
This section discusses a relevant subset of these tools and techniques, and contrasts them with our novel contributions.


\subsection{Comparing Python and compiled code}
\label{sec:python-performance-comparing}

%% Traditional approach is FFI bindings, but this doesn't work for us
% Hook
In his talk ``Python Performance Matters'' \cite{emerybergerPythonPerformanceMatters2022}, Emery Berger argues that although Python is a critical language for writing ``glue code'', its performance is bounded by the overhead of its virtual machine, making it necessarily slower than languages compiled to native machine code.
% Argument
As such, he asserts a maximally performant Python program is one which executes as few Python instructions as possible, instead heavily leveraging code written in more performant languages such as C or FORTRAN through \ac{ffi} bindings.
This approach is effective for many applications, for example libraries such as NumPy and TensorFlow, providing useful abstractions for compiled code over common tasks.
However, these \ac{ffi} bindings incur an overhead, particularly when copying large data structures.
% Link
This makes the approach less suitable for applications which cannot be partitioned into two loosely coupled components split over the language boundary.

%% Our work
% Hook
Our work examines user-extensible compiler frameworks, which we argue contain bottlenecking components such as the pattern rewriter which cannot be split in this way.
% Argument
This is because user defined extensions make frequent interactions across a wide API surface, and operate on large data structures -- resulting in a disproportionate cost from crossing \ac{ffi} boundaries. This contrasts linear algebra workloads such as those supported by NumPy, where large batches of computation are dispatched by only one interaction across the boundary.
In addition to this, compiler optimisation workloads are less amenable to ahead-of-time compiled optimisations such as vectorisations, further reducing the benefit of leveraging low-level compiled languages.
% Link
Our research extends the state-of-the-art by examining these cases where existing best practices are not applicable.


\subsection{Measuring application performance in Python}
\label{sec:python-performance-application}

%% Difficulty measuring python
% Hook
Reliable and accurate performance measurement is notoriously difficult.
As such, its careful execution constitutes the main contribution of systems papers and theses \cite{crapeperformance} \cite{harris2021understanding}.
% Argument
This difficulty comes from both sides of the hardware-software interface.
For example, hardware optimisations such as hierarchical caches, branch predictors, and power management schemes introduce noise, making performance measurements less predictable and consistent. Similar confounding effects come from software, from process scheduling in the operating system to garbage collection in language runtimes.
Beyond this, advanced interpreters leverage runtime performance information for adaptive specialisation and JIT compilation, further muddling measurements. This phenomenon is explored by Barrett et al.'s ``Virtual Machine Warmup Blows Hot and Cold'' \cite{barrettVirtualMachineWarmup2017}, where interpreter virtual machine warmup is shown to be highly variable, with benchmarks taking over 2000 iterations to reach a steady state.
% Link
As such, accurate measurement of the performance characteristics of a Python program is more involved than the na\"ive approach of taking the wall time it takes to execute -- requiring additional tools and techniques to guarantee reliable results.

%% Traditional measurement and profilers
% Hook
Fortunately, Python's strong ecosystem provides a wide variety of tools to achieve this goal.
% Argument
% TODO: possibly needs link like "This existing body of work..."?
The standard library includes the \texttt{timeit} utility, which measures the wall clock time to execute code snippets. However, \texttt{timeit} is fairly simplistic, and does not account for all the complexities of modern interpreted runtimes.
The \texttt{pyperf} \cite{victorstinnerPsfPyperf2025} package aims to address these issues, with more complex control over confounding effects such as warm-ups and CPU isolation.
Other tools provide more specialised functionality, for example Airspeed Velocity \cite{michaeldroettboomAirspeedvelocityAsv2025} visualising performance regressions across repository commits, and \texttt{pyperformance} \cite{collinwinterPythonPyperformance2025} benchmarking the performance of the Python language implementation itself across a variety of workloads.
% Link
Our work leverages these tools to make accurate measurements of compiler framework performance.

%% Our measurements and infrastructure
% Hook
A key contribution of our research is the comprehensive performance measurements and analysis of the xDSL and \ac{mlir} user-extensible compiler frameworks.
% Argument
This builds upon the tools and techniques established by the existing body of work examining the performance of the Python language,
In addition to the research contribution of these measurements themselves, our work further supports ongoing research using the xDSL framework by providing re-usable performance benchmarks and associated tooling to measure performance.
% Link
However, sometimes measurements with finer than end-to-end granularity are required to characterise performance properties more deeply. As such, our tooling also provides a simple user interface for applying performance profilers to these benchmarks.


\subsection{Profiling to understand Python's performance}
\label{sec:python-performance-profiling}

%% TODO: To what degree does this duplicate related work? Will this need hoisting?
%% Research gap
% Hook
Existing profilers for Python typically operate at the function level.
% Argument
For example, Python's standard library provides the \texttt{profile} module, a Python-native tracing profiler, along with \texttt{cProfile}, a more performant C implementation of the same functionality \cite{pythonsoftwarefoundationPythonProfilers}. These instrument each call event, providing accurate profiling information for each evaluated function.
Beyond the standard library, profilers such as \texttt{pyinstrument} use statistical sampling rather than tracing to reduce overhead incurred by performance measurement \cite{rickerbyPyinstrument2025}.
In addition to this, the recent OSDI best paper winner ``Triangulating Python Performance Issues with SCALENE'' \cite{bergerTriangulatingPythonPerformance2023a} introduces another profiler which focusses on the \ac{ffi} boundary between C and Python, a key bottleneck for the best practice of delegating computation to fast low-level implementations.
This delegation is particularly effective for structured workloads such as linear algebra, but is less suitable for highly dynamic workloads.
As such, profilers for pure Python are still important for these applications.
Furthermore, profiling information at a finer granularity than the function level is often needed to deeply the performance of a program.
% Link
\texttt{line\_profiler} provides this functionality to a line level \cite{robertkernPyutilsLine_profiler2025}, but this is still one level of abstraction over the increasingly complex implementation of CPython's interpreter.

% % \vspace{1em}
% \begin{listing}[H]
%     \centering
%     \begin{minted}[fontsize=\footnotesize]{text}
%       214 function calls (207 primitive calls) in 0.002 seconds

% Ordered by: cumulative time

% ncalls  tottime  percall  cumtime  percall filename:lineno(function)
%      1    0.000    0.000    0.002    0.002 {built-in method builtins.exec}
%      1    0.000    0.000    0.001    0.001 <string>:1(<module>)
%      1    0.000    0.000    0.001    0.001 __init__.py:250(compile)
%      1    0.000    0.000    0.001    0.001 __init__.py:289(_compile)
%      1    0.000    0.000    0.000    0.000 _compiler.py:759(compile)
%      1    0.000    0.000    0.000    0.000 _parser.py:937(parse)
%      1    0.000    0.000    0.000    0.000 _compiler.py:598(_code)
%      1    0.000    0.000    0.000    0.000 _parser.py:435(_parse_sub)
%     \end{minted}
%     \vspace{1em}
%     \caption{Profiling results of \texttt{cProfile.run('re.compile("foo|bar")')} are at the granularity of function calls.}
%     \label{listing:cprofile-example}
% \end{listing}


%% Python bytecode and tooling
% Hook
Bytecode is the fundamental unit of Python's interpreted execution loop. As such, tooling to examine bytecode provides deep insight into its underlying operation. % TODO: Make this slightly less on the nose...
% Argument
Python's standard library again provides functionality to interact with this core aspect of the language, through the \texttt{dis} and \texttt{opcode} modules. However, their functionality is limited to outputting only the compiled bytecode for functions, as opposed to the actual bytecodes dispatched at runtime. As such, it cannot capture the control flow followed during execution, nor runtime specialisations made to the bytecode.
% TODO: Could talk about LLTRACE debug capabilities, but only complicates narrative without changing its message
Beyond this, members of the Python community have proposed small proofs of concept for tracing and rewriting bytecode sequences \cite{0xecCodingReversingHacking2017} \cite{clementrouaultUnderstandingPythonExecution}, and leveraged it for improved coverage metrics \cite{nedbatchelderWickedHackPython2008}.
This body existing work covers static code analysis and tracing execution paths at the bytecode level, but does not provide functionality for measuring performance. This missing functionality is critical for our research goals of fully understanding the impact of dynamism on the performance of Python's runtime.
% Link
To unblock this, our work extends the state-of-the-art to support performance profiling of Python bytecode.

%% ByteSight
% Hook
Our novel bytecode performance profiling tool, ByteSight,
% Argument
% Link
