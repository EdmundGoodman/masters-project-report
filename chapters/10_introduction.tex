\chapter{Introduction}
\label{chap:introduction}

% This is the introduction where you should introduce your work. In general the thing to aim for here is to describe a little bit of the context for your work -- why did you do it (motivation), what was the hoped-for outcome (aims) -- as well as trying to give a brief overview of what you actually did.
% It's often useful to bring forward some ``highlights'' into this chapter (e.g.\ some particularly compelling results, or a particularly
% interesting finding).
% It's also traditional to give an outline of the rest of the document, although without care this can appear formulaic and tedious. Your call.




%%% "describe a little bit of the context for your work"

%% Context (~abstract sentence one: topic of compilers/MLIR)
Compilers are a critical component of computing systems, providing an abstraction % for expressivity, performance, and portability (review: first sentence too long)
from high-level programming languages to the underlying machine ISA.
Early compilers were hand-crafted, resulting in complex and language-specific implementations.
The LLVM compiler framework \cite{lattnerLLVMCompilationFramework2004} addressed these problems of complexity and re-usability with a novel and language-independent \ac{ir} in \ac{ssa} form, which could be analysed and transformed by a sequence of passes.
Recently, \ac{mlir} \cite{lattnerMLIRScalingCompiler2021a}, has furthered these goals by enabling users to cheaply extend compilers with new abstractions, and automatically provides common infrastructure such as parsing and printing logic for them.




%%% "why did you do it (motivation)"

%% Context/Motivation (~abstract sentence one/two/three: extensible compiler frameworks, how this impacts performance, and that MLIR accept it)
Compiler extensibility is critical for handling the heterogenous hardware and exotic optimisations of modern workloads.
However, this extensibility comes at a cost, presenting an optimisation boundary between the user and framework code. This boundary arises from the dynamic dispatch of these extensions, precluding optimisations such as ahead-of-time code motion.
This is compounded by the dynamic nature of the data structures and algorithms used by the compiler. For example, pattern rewriting in \ac{mlir} operates on a linked list representation of \ac{ssa} values -- a dynamic, pointer-chasing workload.
This further inhibits optimisations, both as a result of dynamism and not being amenable to other transformations such as vectorisation.
These factors motivate challenging the status quo of LLVM and MLIR implementing user-extensible compiler frameworks in static, ahead-of-time compiled languages.






%%% "what was the hoped-for outcome (aims)"

%% Motivation/aim (~abstract sentence four: why not use dynamic languages/xDSL?)
xDSL \cite{fehrXDSLSidekickCompilation2025} is a reimplementation of \ac{mlir}'s core data structures and \ac{ir} definitions in Python, a dynamically typed, interpreted language.
xDSL's implementation in Python avoids the long build times of \ac{mlir} as a result of being interpreted. %, and its expressive syntax and minimal boilerplate allows compiler designers to focus on their task as opposed to the underlying language and framework.
In addition to this, Python's dynamic typing matches the dynamic nature of the user-extensible compiler framework workload.
However, using Python also has drawbacks, most notably in relation to runtime performance.
This work examines the performance of program optimisation through pattern rewriting in xDSL, focussing on the impact of dynamism and contrasting the current state-of-the-art, \ac{mlir}.





%%% "give a brief overview of what you actually did"


\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{images/introduction/narrative.drawio.pdf}
    \caption{\textit{(Sketch of figure: will be drawn properly with matplotlib and real data once finalised -- any general feedback appreciated!)} Closing the gap between xDSL (blue language runtime, orange implementation overhead) and \ac{mlir} (green) pattern rewriting performance. xDSL's performance can be improved by changes to both its implementation and language runtime. C++ has less of a performance advantage for \ac{mlir} than other workloads due to the costs associated with dynamism.}
    % TODO: Further think about this to consider making it more conceptual/hooky
    % TODO: If kept, this will have bars labelled with performance figures, but since not final have not done this yet
    \label{fig:narrative}
\end{figure}

%% Overview (~abstract sentence five, which needs re-writing: what we did part one -- specialisation and CPython stuff)
The performance of a program is constrained both by the details of its implementation and the runtime of its language.
These two properties are deeply interlinked, making them difficult to measure independently.
To disentangle them, we manually optimise and specialise xDSL's implementation of pattern rewriting (Label \texttt{(1)} of \autoref{fig:narrative}), resulting in an $5.5\times$ performance uplift. % TODO: align with finalised figure when re-measured
We then confirm the performance of the specialisation is constrained only by the language runtime by examination of the dispatched bytecode of micro-benchmarks.
Following this, we quantify the impact of recent performance enhancements made to CPython for this specialised implementation (Label \texttt{(2)} of \autoref{fig:narrative}). % as an increase of $xy\%$.
This process provides an approximate best-case for the performance of pattern rewriting in xDSL, which can then be compared against \ac{mlir}.


%% Overview (~abstract sentence five, which needs re-writing: what we did part two -- custom CPython optimisations)
% Hook
% Argument
% Link


%% Overview (~abstract sentence five, which needs re-writing: what we did part three -- cost of dynamism)
A key difference between the Python and C++ runtimes is their degree of dynamism.
\ac{mlir}'s C++ runtime incurs overhead when dynamically dispatching functions (Label \texttt{(3)} of \autoref{fig:narrative}), which is worsened by prohibiting ahead-of-time performance optimisations. In contrast, almost every bytecode operation evaluated by the Python interpreter is dynamic, each incurring an overhead.
As such, we expect the difference in performance between language runtimes (Label \texttt{(4)} of \autoref{fig:narrative}) to be smaller for more dynamic workloads.
To corroborate this, we measure the difference in performance between pattern rewriting workloads using xDSL and \ac{mlir}, and assess the contribution of overheads incurred by dynamism.
This measurement procedure uniquely leverages xDSL's sidekick compilation functionality to ensure the comparability of performance measurements by driving them with the same textual \ac{ir}, even for implementation details internal to each framework.
Finally, we critically evaluate the degree to which this motivates the use of Python for implementing user-extensible compiler frameworks.



%% Contributions (~abstract sentence six: key impact of research)
The contributions of our work are as follows:

\begin{itemize}
    \item An examination of the best-case performance for pattern rewriting workloads in the CPython language runtime. %, including trade-offs in expressivity and the impact of performance optimisations made to the language runtime.
    \item An exploration of optimisation techniques to shrink the performance gap between dynamic and static languages for pattern rewriting workloads.
    \item A tool to examine CPython bytecode dispatch in program runs, facilitating the analysis of costs incurred by dynamism.
    \item A quantitative comparison of the performance of user-extensible compiler frameworks implemented in static and dynamic languages, focussing on the impact of dynamism. % leveraging sidekick compilation for fine-grained analysis of the impact of dynamism
\end{itemize}
